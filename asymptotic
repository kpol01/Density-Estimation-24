
## Inter-play between $n$ and $h_n$

#### Points to be noted

::: incremental
-   With decrease in $h_n$ results are expected to more precise.
-   The asymptotic variance of $f_n(x)$ is of order $nh_n$, hence $nh_n$ is the right scaling factor for $(f_n(x) - f(x))$.
-   For asymptotic results to hold we need $h_n \rightarrow 0$ and $nh_n \rightarrow \infty$
-   To achieve normality we not only need $n$ to be large but $nh_n$ to be large as well.
-   So depending on the choice of $n$ we need to choose $h_n$. Hence $h_n$ also plays a role in the asymptotic behaviour of $f_n(x)$.
-   To be precise, if we have a small $n$ $h_n$ needs to be of smaller order than $h_n$ required for larger $n$ to attain asymptotic normality.
:::

## Inter-play between $n$ and $h_n$ {.scrollable}

#### Samples from N(0,1), with *Gaussian kernel*, taking $h_n = n^{-0.1}, n^{-0.2}, n^{-0.3}, n^{-0.4}$

```{r, echo = F, fig.height=5, include=F}
library(ggplot2)
library(gridExtra)

# Define the kernel functions
gaussian_kernel <- function(u) dnorm(u)
logistic_kernel <- function(u) dlogis(u)
naive_kernel <- function(u) ifelse(abs(u) <= 1, 0.5, 0)
tricube_kernel <- function(u) ifelse(abs(u) <= 1, 35/32 * (1 - abs(u)^3)^3, 0)
cosine_kernel <- function(u) ifelse(abs(u) <= 1, pi/4 * cos(pi/2 * u), 0)
epanechnikov_kernel <- function(u) ifelse(abs(u) <= 1, 3/4 * (1 - u^2), 0)

# Set the values for n and calculate h_n for each
n_values <- c(20, 50, 100, 250, 500)

n<-5000
n_values <- rep(n, 4)
h_values <- c(n ^ (-0.1), n^(-0.2) , n^(-0.3), n^(-0.4))


# Function to simulate and plot for a given kernel

simulate_and_plot <- function(kernel_func, kernel_name, quantile, n) {
  
  n_values <- rep(n, 4)
  h_values <- c(n ^ (-0.1), n^(-0.2) , n^(-0.3), n^(-0.4))
  p <- list()
  for (i in (1:length(n_values))) {
    n <- n_values[i]
    h <- h_values[i]
    
    # Generate random samples
    
    
    # Compute the 0.1 quantile of the sample
    x_q <- qnorm(quantile, 0, 1)
    
    # For each sample, estimate the density at x_q using the kernel function
    f_n_x <- replicate(1000, {
      sample_x_i <- rnorm(n,0,1)          # New sample for each replication
      mean(kernel_func((x_q - sample_x_i) / h)) / h
    })
    
    
    df<-D(D(expression(1/sqrt(2*pi)*exp(-0.5*x^2)),"x"),"x")
    mode(df)
    x<-x_q
    E_f_n_x <- dnorm(x_q,0,1)+ (0.5 * h^2 * eval(df))
    kernel_int <- integrate(function(u) kernel_func(u)^2, lower = -Inf, upper = Inf)$value
    var_f_n_x <- (1 / (n * h)) * dnorm(x_q,0,1) * kernel_int
    
    # Standardize f_n(x)
    standardized_f_n_x <- (f_n_x-E_f_n_x)/sqrt( var_f_n_x)
    
    # Create a dataframe for plotting
    data_to_plot <- data.frame(standardized_f_n_x = standardized_f_n_x)
    
    # Plot the histogram with the density of N(0,1) overlaid
    
    
    p[[i]] <- ggplot(data_to_plot, aes(x = standardized_f_n_x)) +
      geom_histogram(aes(y = ..density..), bins = 30, color = "black", fill = "skyblue") +
      stat_function(fun = dnorm, args = list(mean = 0, sd = 1), color = "red", size = 1) +
      labs(title = paste("| n =", n, "| h_n =", round(h, 3)),
           x = "Standardized f_n(x_q)", y = "Density")
    
  }
  return(p)
}

p1 <- grid.arrange(grobs = simulate_and_plot(gaussian_kernel, "Gaussian", 0.5, 20), nrow = 1)
p2 <- grid.arrange(grobs = simulate_and_plot(gaussian_kernel, "Gaussian", 0.5, 100), nrow = 1)
p3 <- grid.arrange(grobs = simulate_and_plot(gaussian_kernel, "Gaussian", 0.5, 500), nrow = 1)


```

::: incremental
```{r, echo = F, fig.height=5}
grid.arrange(grobs = list(p1, p2, p3), ncol = 1)
```
:::

## Inter-play between $n$ and $h_n$

#### Observations

:::incremental
- For large $n$ we need $h_n$ of very small orders to attain asymptotic normality.
- For $n = 20$, even $h_n = n ^{-0.4}$ there are some disturbances with the peak of the histograms.
- Where as for $n = 500$, with $h_n = n ^ {-0.2}$ we get similar results that we get with $h_n = n ^ {-0.4} for n = 20$
- $nh_n$ determines the rate of convergence of $f_n(x) - f(x)$ to asymptotic noormality.
:::

